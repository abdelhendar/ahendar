{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fb4fa",
   "metadata": {},
   "source": [
    "# 1. Importation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_json(\"datasetProjet2022.json\")\n",
    "seisme = pd.read_csv('Liste_seismes_2017-2022.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0514b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db83fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e727064",
   "metadata": {},
   "outputs": [],
   "source": [
    "seisme.isna().sum()\n",
    "# Aucun champs n'est null pour les seismes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne garde que quelques features qui vont nous interesser pour la suite\n",
    "tweet=tweet[['hashtags','tweet_text','tweet_created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb02d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trie les lignes de tweet par date croissante\n",
    "tweet=tweet.sort_values('tweet_created_at').reset_index()\n",
    "tweet=tweet.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on d√©fini la date comme le nouvel index\n",
    "tweet=tweet.set_index(tweet['tweet_created_at'])\n",
    "tweet=tweet.drop('tweet_created_at',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b951e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pr√©l√®vement des dates et heure des seismes\n",
    "seisme=seisme['Date Heure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734854af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637172c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrondissement des dates de s√©isme √† la seconde pr√®s\n",
    "seisme=seisme.map(lambda x: x[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a659b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la date en datetime\n",
    "seisme=pd.to_datetime(seisme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du format datetime\n",
    "seisme[0]-pd.Timedelta('5h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338413c",
   "metadata": {},
   "source": [
    "Les op√©rations sur les dates fonctionnement, c'est valid√© !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fda34",
   "metadata": {},
   "source": [
    "# 2. Etiquetage des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91470974",
   "metadata": {},
   "source": [
    "Id√©alement, il faudrait des tweets labellis√©s √† la main. Ce n'est pas le cas ici !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d3c22",
   "metadata": {},
   "source": [
    "# 2.1 Les fenetres de s√©ismes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61124085",
   "metadata": {},
   "source": [
    "Pour labelliser les tweets, on fera l'hypoth√®se qu'un s√©isme a un impact sur tweeter pendant les 12h qui suivent son apparition. Ce pas de temps m√©riterait d'√™tre √©tudier pour trouver la valeur qui optimise les pr√©dictions de notre algorithme de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la fonction construisant les fenetres de tremblement de terre\n",
    "\n",
    "def get_window(seisme,window_hour):\n",
    "    windows=[]\n",
    "    string=str(window_hour)+'h'\n",
    "    for elem in seisme:\n",
    "        windows.append((elem,elem+pd.Timedelta(string)))\n",
    "    return windows\n",
    "\n",
    "# Hypoth√®se : si un s√©isme survient, les 12 heures suivantes font partie de la fenetre temporelle de ce tremblement de terre\n",
    "windows=get_window(seisme,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da660c8",
   "metadata": {},
   "source": [
    "# 2.2 Etiquetage binaire des tweets : \n",
    "\n",
    "On utilise les fenetres de tremblement de terre ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea9ae8",
   "metadata": {},
   "source": [
    "Lab√©llis√©s √† 1 s'ils appartiennent √† une fenetre de tremblement de terre, 0 sinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la feature seism_association\n",
    "tweet['seism_association']=np.zeros(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23676506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les tweets sont lab√©llis√©s\n",
    "for elem in tweet.index:\n",
    "    for elem2 in windows:\n",
    "        if elem > elem2[0] and elem < elem2[1]:\n",
    "            tweet.loc[elem,'seism_association']=1  # Si le tweet est dans une fenetre de tremblement de terre, il est positif\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de tweets √©tiquet√©s √† 1\n",
    "tweet['seism_association'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d459e",
   "metadata": {},
   "source": [
    "31 423 tweets sont positifs. Cela repr√©sente une faible part des 500 000 tweets. Il faut r√©√©quilibrer le dataset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les tweets positifs\n",
    "tweet[tweet['seism_association']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984b95a",
   "metadata": {},
   "source": [
    "Certains tweets sont labellis√©s positifs alors qu'ils ne parlent pas d'un vrai s√©isme. On devra le prendre en compte dans l'√©tude des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de tremblement de terre mal labellis√©\n",
    "tweet_positif = tweet[tweet['seism_association']==1]\n",
    "tweet_positif.iloc[13].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534537a0",
   "metadata": {},
   "source": [
    "# 3. R√©√©quilibrer le dataset de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer une bonne part des tweets n√©gatifs\n",
    "list_suppr=[]  # liste de tweets √† supprimer\n",
    "i=0\n",
    "while i<len(tweet):\n",
    "    if tweet.seism_association[i]==0:  # si le tweet est labelis√© \"negatif\"\n",
    "        alea = random.random()\n",
    "        if alea>=0.20: # On jette au hazard 80% des tweets\n",
    "            list_suppr.append(tweet.index[i])\n",
    "    i=i+1        \n",
    "tweet.drop(list_suppr,0,inplace=True)\n",
    "len(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c714a5",
   "metadata": {},
   "source": [
    "Il reste environ 125 000 tweets dont 32 000 lab√©llis√©s positifs. Jetter de fa√ßon al√©atoire des tweets pourrait causer des probl√®mes de fr√©quences de tweets dans notre mod√®le de ML. On garde cela en t√™te."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa164cbf",
   "metadata": {},
   "source": [
    "# 4. Nettoyer le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les caract√®res avec accents par les lettres correspondantes\n",
    "import re\n",
    "from datetime import datetime\n",
    "from thefuzz import fuzz\n",
    "\n",
    "\n",
    "def de_accentize(text):\n",
    "    \"\"\"\n",
    "    Remove usual latin language accentuation from letters (uppercase as well as lowercase)\n",
    "    \"\"\"\n",
    "    accentedChars =    '√†√Ä√£√É√©√â√®√à√´√ã√≠√ç√Æ√é√≥√ì√µ√ï√¥√î√∫√ö√ª√õ√π√ô√±√ë√á√ß'\n",
    "    de_accentedChars = 'aaaaeeeeeeiiiioooooouuuuuunncc'\n",
    "    transTable = str.maketrans(accentedChars,de_accentedChars)\n",
    "    return text.translate(transTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer la feature texte (tout en minuscule, pas de caract√®re sp√©cial,...)\n",
    "texte_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    texte = tweet['tweet_text'][i]\n",
    "    texte=de_accentize(texte)\n",
    "    texte = texte.lower()\n",
    "    texte = re.sub('((www\\.[\\s]+)|(https?://[^s]+))','', texte)\n",
    "    texte = re.sub(\"@[A-Za-z0-9_]+\",\"\", texte)\n",
    "    texte = re.sub(\"#[A-Za-z0-9_]+\",\"\", texte)\n",
    "    texte = re.sub('[()!?]', ' ', texte)\n",
    "    texte = re.sub('\\[.*?\\]',' ', texte)\n",
    "    texte = re.sub(\"[^a-z0-9]\",\" \", texte)\n",
    "    texte_list.append(texte)\n",
    "tweet['texte_nettoye'] = texte_list\n",
    "tweet['texte_nettoye']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6904ae0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ba68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144aa566",
   "metadata": {},
   "source": [
    "# 5. Cr√©er de nouvelles features √† partir du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ae229",
   "metadata": {},
   "source": [
    "# 5.1 Feature nombre de mots par tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de la feature nombre de mot par tweet\n",
    "nb_mot_list=[]\n",
    "for phrase in tweet['texte_nettoye']:\n",
    "    nb_mot_list.append(len(phrase.split()))\n",
    "# Mettre √† jour la feature avec le nombre de mots\n",
    "tweet['nb_mot']=nb_mot_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a248a56",
   "metadata": {},
   "source": [
    "# 5.2 Features mots relatifs au champs lexical du s√©isme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction concat√©nant le texte d'un vecteur\n",
    "def unpack(L):\n",
    "    unpacked=''\n",
    "    for i in range (len(L)):\n",
    "        unpacked+=L[i]\n",
    "    return unpacked\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef03bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat√©nation du texte des tweets positifs\n",
    "unpack(text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341881d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dictionnaire de tokens et fr√©quence\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer=TweetTokenizer()\n",
    "tokens=tokenizer.tokenize(unpack(text_valid))\n",
    "freq=nltk.FreqDist(tokens)\n",
    "for w in sorted(freq, key=freq.get, reverse=True):\n",
    "  print (w, freq[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212cf7",
   "metadata": {},
   "source": [
    "Dans ce dictionnaire de tokens, on peut retrouver des mots du champs lexical du s√©isme tel que \"seisme\", \"magnitude\", \"tremblements\" et \"terre\" avec des occurences importantes. On peut donc cr√©er des features pour chacun de ces mots cl√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features du champs l√©xical du s√©isme\n",
    "mot_seisme=np.zeros(len(tweet))\n",
    "mot_tremblement=np.zeros(len(tweet))\n",
    "mot_terre=np.zeros(len(tweet))\n",
    "mot_magnitude=np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    tok=tokenizer.tokenize(tweet.texte_nettoye[i])\n",
    "    if \"seisme\" in tok or \"seismes\" in tok:\n",
    "        mot_seisme[i]=1\n",
    "    if \"tremblement\" in tok or \"tremblements\" in tok:\n",
    "        mot_tremblement[i]=1\n",
    "    if \"terre\" in tok:\n",
    "        mot_terre[i]=1\n",
    "    if \"magnitude\" in tok:\n",
    "        mot_magnitude[i]=1\n",
    "tweet['mot_seisme']=mot_seisme\n",
    "tweet['mot_tremblement']=mot_tremblement\n",
    "tweet['mot_terre']=mot_terre\n",
    "tweet['mot_magnitude']=mot_magnitude\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du texte du 11eme tweet du dataframe\n",
    "tweet.iloc[11].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du 11eme tweet\n",
    "tweet.iloc[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453ffa9",
   "metadata": {},
   "source": [
    "On observe que le texte du tweet et les features \"mot_...\" sont en ad√©quation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00e68d",
   "metadata": {},
   "source": [
    "# 5.3 Feature sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71619732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'analyse de sentiment\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "texte_nettoye = \"C'est incroyable j'ai r√©ussi √† relever le plus gros d√©fi de ma vie je suis tellement heureux\"\n",
    "sentiment = TextBlob(texte_nettoye,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment[0]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702635ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la feature sentiment\n",
    "tweet['sentiment']=np.zeros(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cr√©er la feature sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "emotion = np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    emotion[i] = TextBlob(tweet.texte_nettoye[i],pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment[0]\n",
    "tweet.sentiment = emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434307a",
   "metadata": {},
   "source": [
    "# 5.4 Feature fr√©quence\n",
    "\n",
    "Attention : La cellule ci dessous met 6H √† tourner (sur mon PC...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcd295",
   "metadata": {},
   "source": [
    "En effet cette feature a une complexit√© en 2n¬≤, il y a environ 125 000 tweets ce qui implique un temps de calcul tr√®s long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fr√©quence locale d'un tweet, c'est le nombre de tweets publi√©s dans un \"rayon\" de 12h autour de la date du tweet\n",
    "\n",
    "# Initialiser la feature frequence\n",
    "tweet['frequence']=np.zeros(len(tweet))\n",
    "# initialiser la liste des fr√©quences\n",
    "freq_list=np.zeros(len(tweet))\n",
    "delta= '12h'  \n",
    "for i in range(len(tweet)):\n",
    "    time_tweet=tweet.index[i] #Date de publication du tweet\n",
    "    debut_window=time_tweet-pd.Timedelta(delta) # Date de d√©but de la fenetre locale du tweet\n",
    "    fin_window=time_tweet+pd.Timedelta(delta) # Date de fin de la fenetre locale du tweet\n",
    "    df1=tweet[tweet.index>=debut_window] # Pr√©l√®vement des tweets dont la date est sup√©rieure √† la date de d√©but\n",
    "    df2=tweet[tweet.index<fin_window]  # Pr√©l√®vement des tweets dont la date est inf√®rieure √† la date de fin\n",
    "    df3=pd.merge(df1,df2) # Intersection des 2 tableaux ci-dessus\n",
    "    freq=len(df3) # frequence locale calcul√©e pour le tweet en question\n",
    "    freq_list[i]=freq\n",
    "    print(\"i=\"+str(i)) # Cet affichage permet de v√©rifier que le programme tourne toujours apr√®s quelques heures....\n",
    "    print(freq)\n",
    "    \n",
    "tweet['frequence']=freq_list\n",
    "tweet['frequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la frequence locale d'un tweet\n",
    "tweet['frequence'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de frequence locale en fonction du temps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tweet.index, tweet.frequence)\n",
    "plt.ylabel('Fr√©quence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac746b",
   "metadata": {},
   "source": [
    "Globalement, on observe une √©trange gaussienne pour la r√©partition des tweets au cours des ann√©es. On peut supposer que le covid a entrain√© une augmentation des tweets. Les diff√©rents pics peuvent √™tre du √† la suppression al√©atoire des tweets n√©gatifs op√©r√©e plus haut ou alors √† l'apparition d'un tremblement de terre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(tweet.index[47000:72000], tweet.frequence[47000:72000])\n",
    "plt.ylabel('Fr√©quence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd4f81",
   "metadata": {},
   "source": [
    "En regardant la liste des s√©ismes, on retrouve bien un s√©isme en fin juin 2019 et en novembre 2019 ce qui correspond √† nos pics de tweets sur la courbe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91716827",
   "metadata": {},
   "source": [
    "Il serait interessant de d√©finir une nouvelle feature : la variation de la fr√©quence qui permettrait de mettre en valeur les augmentations soudaines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4f0ee",
   "metadata": {},
   "source": [
    "# 5.5 Feature √©cart entre fr√©quence locale et la fr√©quence au loin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b37339",
   "metadata": {},
   "source": [
    "Id√©alement, on devrait calculer un taux d'accroissement pour la frequence pour approximer la d√©riv√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6f642",
   "metadata": {},
   "source": [
    "Faute de temps, on fera une approximation grossi√®re en ne s'interessant qu'√† l'√©cart entre la fr√©quence locale et la fr√©quence au loin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9ed5f",
   "metadata": {},
   "source": [
    "hypoth√®se : la fr√©quence au loin pour le tweet i, c'est la moyenne entre la fr√©quence de tweet[i-pas] et tweet[i+pas]. Apr√®s plusieurs tests, pas=550 semble etre le parametre optimal pour cette feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_freq=np.zeros(len(tweet))\n",
    "pas = 550\n",
    "for i in range(len(tweet)):\n",
    "    if i<pas:\n",
    "        moy_freq = (tweet.frequence[i-pas]+tweet.frequence[i+pas])/2\n",
    "    else:\n",
    "        moy_freq = (tweet.frequence[i-pas]+tweet.frequence[i+pas-len(tweet)])/2\n",
    "    variation_freq[i]=abs(tweet.frequence[i]-moy_freq)\n",
    "tweet['variation_freq']=derive_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de frequence locale en fonction du temps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tweet.index, tweet.variation_freq)\n",
    "plt.ylabel('Variation de la frequence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c19632",
   "metadata": {},
   "source": [
    "# 5.6 Feature nombre d √©mojis par tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e721b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pour compter les emojis\n",
    "import advertools as adv\n",
    "text_list = ['I feel like playing basketball üèÄ',\n",
    "             'I like playing football ‚öΩ‚öΩ',\n",
    "             'Not feeling like sports today']\n",
    "\n",
    "emoji_summary = adv.extract_emoji(text_list)\n",
    "print(emoji_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de la feature repr√©sentant le nombre d'√©moji de chaque tweet\n",
    "nb_emoj=np.zeros(len(tweet))\n",
    "tweet['nb_emoji']=np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    emoji_summary = adv.extract_emoji(tweet.tweet_text[i])\n",
    "    nb_emoj[i] = emoji_summary['overview']['num_emoji']\n",
    "    print(i)\n",
    "tweet['nb_emoji'] = nb_emoj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68990e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[tweet.mot_magnitude==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42145105",
   "metadata": {},
   "source": [
    "# 6 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de chaque variable sous forme d'un histogramme\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweet_clean=tweet[['nb_mot', 'frequence', 'nb_emoji', 'variation_freq', 'sentiment','mot_seisme','mot_tremblement', 'mot_terre', 'mot_magnitude', 'seism_association']]\n",
    "tweet_clean.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06ddb0",
   "metadata": {},
   "source": [
    "La feature sentiment reste majoritairement aux alentours de 0 avec beaucoup de valeures nulles\n",
    "\n",
    "L'√©quilibre relatif entre les 1 et les 0 des features de mots est r√©jouissant.\n",
    "\n",
    "On peut voir que le nombre de tweets positifs repr√©sente 1/5 des tweets via la feature seism_association, et on se f√©licite d'avoir tent√© d'√©quilibrer le jeu de donn√©s.\n",
    "\n",
    "On peut voir √©galement que le nombre de mots suit une distribution de gauss.\n",
    "\n",
    "La fr√©quence est r√©partie de fa√ßon homog√®ne tandis que la variation de la fr√©quence est une demi gaussienne (du √† la valeur absolue).\n",
    "\n",
    "Les donn√©es nous conviennent on peut maintenant passer √† l'√©tude des corr√©lations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice des coefficients de Pearson \n",
    "corr_matrix=tweet_clean.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a417be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice des coefficients de Pearson\n",
    "import seaborn as sns\n",
    "heat_map = sns.heatmap(corr_matrix, center=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3820c8",
   "metadata": {},
   "source": [
    "La fr√©quence est la feature qui pr√©sente le plus de cor√©lation avec les labels (seism_association) avec corr=0,5. Ca sera donc une feature tr√®s interressante pour pr√©dire les tremblements de terre et on pouvait s'y attendre.\n",
    "\n",
    "Les features concernant le champs lexical du s√©isme sont corr√©l√©s entre elles et √ßa aussi c'est coh√©rent. La pr√©sence des mots \"tremblements\" et \"terre\" est fortement corr√©l√© aux labels. Pour les mots magnitude et s√©isme, on retrouve peu de corr√©lation ce qui s'explique par le fait que sur tweeter, le langage soutenu est moins utilis√©. D'un autre cot√©, les tweets √©tant mal √©tiquet√©s, on pourrait s'attendre √† voir les scores de corr√©lation augmenter pour beaucoup des features ci dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d7fcc",
   "metadata": {},
   "source": [
    "# 7 Normalisation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b73239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des features √† normaliser\n",
    "X = tweet_clean.drop('seism_association', axis=1)\n",
    "y = tweet_clean['seism_association']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ced35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # normalisation\n",
    "scaler.fit(X)\n",
    "X_stand = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9da63",
   "metadata": {},
   "source": [
    "# 8. METHODE RANDOMFOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7cf43",
   "metadata": {},
   "source": [
    "Initialisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des entr√©es du mod√®le\n",
    "X_train, X_test, y_train, y_test = X_stand[:100000], X_stand[100001:], y[:100000] , y[100001:]\n",
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a86f67",
   "metadata": {},
   "source": [
    "Entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du mod√®le\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators= 40) # choix de l'hyper param√®tre : 40 = Nombre d'arbres de la foret\n",
    "clf.fit(X_train, y_train) # entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a398a",
   "metadata": {},
   "source": [
    "Pr√©diction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c441ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©diction sur les X_test √† partir de notre mod√®le entrain√©\n",
    "y_predict=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de tweets correspondant √† un tremblement de terre selon notre mod√®le\n",
    "y_predict.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98cff6",
   "metadata": {},
   "source": [
    "La pr√©diction donne 1 259 tweets positifs contre 3 977 pour les y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6ff30",
   "metadata": {},
   "source": [
    "Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des m√©triques pour notre mod√®le entrain√©\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(y_test, y_predict))\n",
    "print(recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fab05",
   "metadata": {},
   "source": [
    "25% de pr√©cision c'est clairement un score pourris. On peut supposer que si les tweets √©taient lab√©llis√©s √† la main le r√©sultat serait meilleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb44c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict[2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fa459",
   "metadata": {},
   "source": [
    "On observe que le tweet 2 000 a √©t√© mal classifi√© par notre mod√®le. C'est tout √† fait normal vu la faible pr√©cision de notre mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173d97a",
   "metadata": {},
   "source": [
    "# Pour synth√©tiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970b829",
   "metadata": {},
   "source": [
    "On a r√©ussi a labelliser les tweets de fa√ßon grossi√®re mais les r√©sultats ne seront pas bon tant que les tweets ne seront pas labellis√©s de fa√ßon plus precise. On pourrait imaginer classifier √† la main les 32 000 tweets labellis√©s positifs par exemple. Sinon, lab√©lliser 32 000 tweets √ßa coute environ 1 300‚Ç¨ donc c'est abordable.\n",
    "\n",
    "La labellisation des tweets fait apparaitre un param√®tre : le temps d'impact d'un s√©isme sur tweeter. Il vaut 12h dans ce mod√®le mais cette valeure n'est pas forc√©ment celle optimale.\n",
    "\n",
    "Un r√©√©quilibrage du dataset a permit de travailler avec un dataset pr√©sentant 20% de tweets positifs.\n",
    "\n",
    "La feature fr√©quence fait apparaitre un autre param√®tre : la dur√©e autour du tweet pour le calcul de sa fr√©quence. Elle vaut 12h √©galement et ce param√®tre peut √™tre optimis√©. Malheureusement, cette feature a un temps de calcul tr√®s long mais doit pouvoir s'optimiser facilement. Nous n'avons pas r√©ussi √† diminuer le temps de calcul en dessous de 6h de notre cot√©.\n",
    "\n",
    "La feature variation de fr√©quence est calcul√©e par une approximation grossi√®re qui ferait peur √† un mat√©maticien. Cependant elle nous a permis, une fois impl√©ment√©, de passer de 7% d'accuracy √† 30% pour certains entrainements de randomforest. Elle fait apparaitre un nouveau param√®tre : le pas de calcul pour la variation de fr√©quence. Il vaut 550 actuellement. Il serait encore plus interessant de remplacer cette feature par un taux d'accroissement. \n",
    "\n",
    "Les features les plus interressantes sont la fr√©quence, la variation de la fr√©quence, la pr√©sence de mots du champs l√©xical du s√©isme. On pourrait √©tudier d'avantage le texte pour en extraire d'autres features interessantes. Cependant, on remarque que les features telles que le nombre de mots, le nombre d'√©mojis ou l'analyse de sentiment n'apporte pas vraiment de r√©sultat concluant. En effet, la correlation entre ces features et la pr√©sence d'un s√©isme est tr√®s proche de 0.\n",
    "\n",
    "Notre mod√®le ne nous permet pas de classifier les tweets de fa√ßon concluante et nous obtenons au mieux 30% de pr√©dictions justes mais on peut esp√©rer qu'avec une lab√©llisation plus juste on pourrait obtenir de meilleures pr√©dictions.\n",
    "\n",
    "La piste de recherche la plus prommeteuse serait de passer en Deep Learning en utilisant un r√©seau de neurones r√©curent tel que un r√©seau LSTM : Chaque tweet poss√®de un texte. Une fois nettoy√©, ce texte est transform√© en un vecteur via de l'embedding. On a donc une s√©quence de mots transform√©s en une s√©quence de nombres. Ainsi, on peut fournir en entr√©e du r√©seau LSTM les vecteurs qui sont des s√©quences correspondant au texte des tweets. Au bout du r√©seau LSTM, on met une couche de neuronnes dense permettant de classifier de facon binaire le tweets. \n",
    "\n",
    "Cette solution permettrait de traduire tout le texte de chaque tweet en tenant compte de l'ordre des mots des phrases qui est tr√®s important pour leur compr√©hension.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
