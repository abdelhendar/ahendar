{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fb4fa",
   "metadata": {},
   "source": [
    "# 1. Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_json(\"datasetProjet2022.json\")\n",
    "seisme = pd.read_csv('Liste_seismes_2017-2022.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0514b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db83fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e727064",
   "metadata": {},
   "outputs": [],
   "source": [
    "seisme.isna().sum()\n",
    "# Aucun champs n'est null pour les seismes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ne garde que quelques features qui vont nous interesser pour la suite\n",
    "tweet=tweet[['hashtags','tweet_text','tweet_created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb02d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trie les lignes de tweet par date croissante\n",
    "tweet=tweet.sort_values('tweet_created_at').reset_index()\n",
    "tweet=tweet.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on défini la date comme le nouvel index\n",
    "tweet=tweet.set_index(tweet['tweet_created_at'])\n",
    "tweet=tweet.drop('tweet_created_at',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b951e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prélèvement des dates et heure des seismes\n",
    "seisme=seisme['Date Heure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734854af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637172c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arrondissement des dates de séisme à la seconde près\n",
    "seisme=seisme.map(lambda x: x[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a659b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la date en datetime\n",
    "seisme=pd.to_datetime(seisme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du format datetime\n",
    "seisme[0]-pd.Timedelta('5h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338413c",
   "metadata": {},
   "source": [
    "Les opérations sur les dates fonctionnement, c'est validé !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fda34",
   "metadata": {},
   "source": [
    "# 2. Etiquetage des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91470974",
   "metadata": {},
   "source": [
    "Idéalement, il faudrait des tweets labellisés à la main. Ce n'est pas le cas ici !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d3c22",
   "metadata": {},
   "source": [
    "# 2.1 Les fenetres de séismes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61124085",
   "metadata": {},
   "source": [
    "Pour labelliser les tweets, on fera l'hypothèse qu'un séisme a un impact sur tweeter pendant les 12h qui suivent son apparition. Ce pas de temps mériterait d'être étudier pour trouver la valeur qui optimise les prédictions de notre algorithme de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la fonction construisant les fenetres de tremblement de terre\n",
    "\n",
    "def get_window(seisme,window_hour):\n",
    "    windows=[]\n",
    "    string=str(window_hour)+'h'\n",
    "    for elem in seisme:\n",
    "        windows.append((elem,elem+pd.Timedelta(string)))\n",
    "    return windows\n",
    "\n",
    "# Hypothèse : si un séisme survient, les 12 heures suivantes font partie de la fenetre temporelle de ce tremblement de terre\n",
    "windows=get_window(seisme,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bffb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da660c8",
   "metadata": {},
   "source": [
    "# 2.2 Etiquetage binaire des tweets : \n",
    "\n",
    "On utilise les fenetres de tremblement de terre ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea9ae8",
   "metadata": {},
   "source": [
    "Labéllisés à 1 s'ils appartiennent à une fenetre de tremblement de terre, 0 sinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la feature seism_association\n",
    "tweet['seism_association']=np.zeros(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23676506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les tweets sont labéllisés\n",
    "for elem in tweet.index:\n",
    "    for elem2 in windows:\n",
    "        if elem > elem2[0] and elem < elem2[1]:\n",
    "            tweet.loc[elem,'seism_association']=1  # Si le tweet est dans une fenetre de tremblement de terre, il est positif\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de tweets étiquetés à 1\n",
    "tweet['seism_association'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d459e",
   "metadata": {},
   "source": [
    "31 423 tweets sont positifs. Cela représente une faible part des 500 000 tweets. Il faut rééquilibrer le dataset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les tweets positifs\n",
    "tweet[tweet['seism_association']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984b95a",
   "metadata": {},
   "source": [
    "Certains tweets sont labellisés positifs alors qu'ils ne parlent pas d'un vrai séisme. On devra le prendre en compte dans l'étude des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de tremblement de terre mal labellisé\n",
    "tweet_positif = tweet[tweet['seism_association']==1]\n",
    "tweet_positif.iloc[13].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534537a0",
   "metadata": {},
   "source": [
    "# 3. Rééquilibrer le dataset de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer une bonne part des tweets négatifs\n",
    "list_suppr=[]  # liste de tweets à supprimer\n",
    "i=0\n",
    "while i<len(tweet):\n",
    "    if tweet.seism_association[i]==0:  # si le tweet est labelisé \"negatif\"\n",
    "        alea = random.random()\n",
    "        if alea>=0.20: # On jette au hazard 80% des tweets\n",
    "            list_suppr.append(tweet.index[i])\n",
    "    i=i+1        \n",
    "tweet.drop(list_suppr,0,inplace=True)\n",
    "len(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c714a5",
   "metadata": {},
   "source": [
    "Il reste environ 125 000 tweets dont 32 000 labéllisés positifs. Jetter de façon aléatoire des tweets pourrait causer des problèmes de fréquences de tweets dans notre modèle de ML. On garde cela en tête."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa164cbf",
   "metadata": {},
   "source": [
    "# 4. Nettoyer le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les caractères avec accents par les lettres correspondantes\n",
    "import re\n",
    "from datetime import datetime\n",
    "from thefuzz import fuzz\n",
    "\n",
    "\n",
    "def de_accentize(text):\n",
    "    \"\"\"\n",
    "    Remove usual latin language accentuation from letters (uppercase as well as lowercase)\n",
    "    \"\"\"\n",
    "    accentedChars =    'àÀãÃéÉèÈëËíÍîÎóÓõÕôÔúÚûÛùÙñÑÇç'\n",
    "    de_accentedChars = 'aaaaeeeeeeiiiioooooouuuuuunncc'\n",
    "    transTable = str.maketrans(accentedChars,de_accentedChars)\n",
    "    return text.translate(transTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer la feature texte (tout en minuscule, pas de caractère spécial,...)\n",
    "texte_list=[]\n",
    "for i in range(len(tweet)):\n",
    "    texte = tweet['tweet_text'][i]\n",
    "    texte=de_accentize(texte)\n",
    "    texte = texte.lower()\n",
    "    texte = re.sub('((www\\.[\\s]+)|(https?://[^s]+))','', texte)\n",
    "    texte = re.sub(\"@[A-Za-z0-9_]+\",\"\", texte)\n",
    "    texte = re.sub(\"#[A-Za-z0-9_]+\",\"\", texte)\n",
    "    texte = re.sub('[()!?]', ' ', texte)\n",
    "    texte = re.sub('\\[.*?\\]',' ', texte)\n",
    "    texte = re.sub(\"[^a-z0-9]\",\" \", texte)\n",
    "    texte_list.append(texte)\n",
    "tweet['texte_nettoye'] = texte_list\n",
    "tweet['texte_nettoye']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6904ae0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ba68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144aa566",
   "metadata": {},
   "source": [
    "# 5. Créer de nouvelles features à partir du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ae229",
   "metadata": {},
   "source": [
    "# 5.1 Feature nombre de mots par tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la feature nombre de mot par tweet\n",
    "nb_mot_list=[]\n",
    "for phrase in tweet['texte_nettoye']:\n",
    "    nb_mot_list.append(len(phrase.split()))\n",
    "# Mettre à jour la feature avec le nombre de mots\n",
    "tweet['nb_mot']=nb_mot_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a248a56",
   "metadata": {},
   "source": [
    "# 5.2 Features mots relatifs au champs lexical du séisme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction concaténant le texte d'un vecteur\n",
    "def unpack(L):\n",
    "    unpacked=''\n",
    "    for i in range (len(L)):\n",
    "        unpacked+=L[i]\n",
    "    return unpacked\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef03bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation du texte des tweets positifs\n",
    "unpack(text_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341881d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire de tokens et fréquence\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer=TweetTokenizer()\n",
    "tokens=tokenizer.tokenize(unpack(text_valid))\n",
    "freq=nltk.FreqDist(tokens)\n",
    "for w in sorted(freq, key=freq.get, reverse=True):\n",
    "  print (w, freq[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212cf7",
   "metadata": {},
   "source": [
    "Dans ce dictionnaire de tokens, on peut retrouver des mots du champs lexical du séisme tel que \"seisme\", \"magnitude\", \"tremblements\" et \"terre\" avec des occurences importantes. On peut donc créer des features pour chacun de ces mots clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features du champs léxical du séisme\n",
    "mot_seisme=np.zeros(len(tweet))\n",
    "mot_tremblement=np.zeros(len(tweet))\n",
    "mot_terre=np.zeros(len(tweet))\n",
    "mot_magnitude=np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    tok=tokenizer.tokenize(tweet.texte_nettoye[i])\n",
    "    if \"seisme\" in tok or \"seismes\" in tok:\n",
    "        mot_seisme[i]=1\n",
    "    if \"tremblement\" in tok or \"tremblements\" in tok:\n",
    "        mot_tremblement[i]=1\n",
    "    if \"terre\" in tok:\n",
    "        mot_terre[i]=1\n",
    "    if \"magnitude\" in tok:\n",
    "        mot_magnitude[i]=1\n",
    "tweet['mot_seisme']=mot_seisme\n",
    "tweet['mot_tremblement']=mot_tremblement\n",
    "tweet['mot_terre']=mot_terre\n",
    "tweet['mot_magnitude']=mot_magnitude\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du texte du 11eme tweet du dataframe\n",
    "tweet.iloc[11].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du 11eme tweet\n",
    "tweet.iloc[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453ffa9",
   "metadata": {},
   "source": [
    "On observe que le texte du tweet et les features \"mot_...\" sont en adéquation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00e68d",
   "metadata": {},
   "source": [
    "# 5.3 Feature sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71619732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'analyse de sentiment\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "texte_nettoye = \"C'est incroyable j'ai réussi à relever le plus gros défi de ma vie je suis tellement heureux\"\n",
    "sentiment = TextBlob(texte_nettoye,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment[0]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702635ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la feature sentiment\n",
    "tweet['sentiment']=np.zeros(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Créer la feature sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "emotion = np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    emotion[i] = TextBlob(tweet.texte_nettoye[i],pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment[0]\n",
    "tweet.sentiment = emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434307a",
   "metadata": {},
   "source": [
    "# 5.4 Feature fréquence\n",
    "\n",
    "Attention : La cellule ci dessous met 6H à tourner (sur mon PC...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcd295",
   "metadata": {},
   "source": [
    "En effet cette feature a une complexité en 2n², il y a environ 125 000 tweets ce qui implique un temps de calcul très long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fréquence locale d'un tweet, c'est le nombre de tweets publiés dans un \"rayon\" de 12h autour de la date du tweet\n",
    "\n",
    "# Initialiser la feature frequence\n",
    "tweet['frequence']=np.zeros(len(tweet))\n",
    "# initialiser la liste des fréquences\n",
    "freq_list=np.zeros(len(tweet))\n",
    "delta= '12h'  \n",
    "for i in range(len(tweet)):\n",
    "    time_tweet=tweet.index[i] #Date de publication du tweet\n",
    "    debut_window=time_tweet-pd.Timedelta(delta) # Date de début de la fenetre locale du tweet\n",
    "    fin_window=time_tweet+pd.Timedelta(delta) # Date de fin de la fenetre locale du tweet\n",
    "    df1=tweet[tweet.index>=debut_window] # Prélèvement des tweets dont la date est supérieure à la date de début\n",
    "    df2=tweet[tweet.index<fin_window]  # Prélèvement des tweets dont la date est infèrieure à la date de fin\n",
    "    df3=pd.merge(df1,df2) # Intersection des 2 tableaux ci-dessus\n",
    "    freq=len(df3) # frequence locale calculée pour le tweet en question\n",
    "    freq_list[i]=freq\n",
    "    print(\"i=\"+str(i)) # Cet affichage permet de vérifier que le programme tourne toujours après quelques heures....\n",
    "    print(freq)\n",
    "    \n",
    "tweet['frequence']=freq_list\n",
    "tweet['frequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la frequence locale d'un tweet\n",
    "tweet['frequence'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de frequence locale en fonction du temps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tweet.index, tweet.frequence)\n",
    "plt.ylabel('Fréquence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac746b",
   "metadata": {},
   "source": [
    "Globalement, on observe une étrange gaussienne pour la répartition des tweets au cours des années. On peut supposer que le covid a entrainé une augmentation des tweets. Les différents pics peuvent être du à la suppression aléatoire des tweets négatifs opérée plus haut ou alors à l'apparition d'un tremblement de terre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(tweet.index[47000:72000], tweet.frequence[47000:72000])\n",
    "plt.ylabel('Fréquence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd4f81",
   "metadata": {},
   "source": [
    "En regardant la liste des séismes, on retrouve bien un séisme en fin juin 2019 et en novembre 2019 ce qui correspond à nos pics de tweets sur la courbe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91716827",
   "metadata": {},
   "source": [
    "Il serait interessant de définir une nouvelle feature : la variation de la fréquence qui permettrait de mettre en valeur les augmentations soudaines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4f0ee",
   "metadata": {},
   "source": [
    "# 5.5 Feature écart entre fréquence locale et la fréquence au loin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b37339",
   "metadata": {},
   "source": [
    "Idéalement, on devrait calculer un taux d'accroissement pour la frequence pour approximer la dérivée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6f642",
   "metadata": {},
   "source": [
    "Faute de temps, on fera une approximation grossière en ne s'interessant qu'à l'écart entre la fréquence locale et la fréquence au loin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9ed5f",
   "metadata": {},
   "source": [
    "hypothèse : la fréquence au loin pour le tweet i, c'est la moyenne entre la fréquence de tweet[i-pas] et tweet[i+pas]. Après plusieurs tests, pas=550 semble etre le parametre optimal pour cette feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_freq=np.zeros(len(tweet))\n",
    "pas = 550\n",
    "for i in range(len(tweet)):\n",
    "    if i<pas:\n",
    "        moy_freq = (tweet.frequence[i-pas]+tweet.frequence[i+pas])/2\n",
    "    else:\n",
    "        moy_freq = (tweet.frequence[i-pas]+tweet.frequence[i+pas-len(tweet)])/2\n",
    "    variation_freq[i]=abs(tweet.frequence[i]-moy_freq)\n",
    "tweet['variation_freq']=derive_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de frequence locale en fonction du temps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tweet.index, tweet.variation_freq)\n",
    "plt.ylabel('Variation de la frequence des tweets en fonction du temps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c19632",
   "metadata": {},
   "source": [
    "# 5.6 Feature nombre d émojis par tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e721b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pour compter les emojis\n",
    "import advertools as adv\n",
    "text_list = ['I feel like playing basketball 🏀',\n",
    "             'I like playing football ⚽⚽',\n",
    "             'Not feeling like sports today']\n",
    "\n",
    "emoji_summary = adv.extract_emoji(text_list)\n",
    "print(emoji_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la feature représentant le nombre d'émoji de chaque tweet\n",
    "nb_emoj=np.zeros(len(tweet))\n",
    "tweet['nb_emoji']=np.zeros(len(tweet))\n",
    "for i in range(len(tweet)):\n",
    "    emoji_summary = adv.extract_emoji(tweet.tweet_text[i])\n",
    "    nb_emoj[i] = emoji_summary['overview']['num_emoji']\n",
    "    print(i)\n",
    "tweet['nb_emoji'] = nb_emoj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68990e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[tweet.mot_magnitude==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42145105",
   "metadata": {},
   "source": [
    "# 6 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de chaque variable sous forme d'un histogramme\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweet_clean=tweet[['nb_mot', 'frequence', 'nb_emoji', 'variation_freq', 'sentiment','mot_seisme','mot_tremblement', 'mot_terre', 'mot_magnitude', 'seism_association']]\n",
    "tweet_clean.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06ddb0",
   "metadata": {},
   "source": [
    "La feature sentiment reste majoritairement aux alentours de 0 avec beaucoup de valeures nulles\n",
    "\n",
    "L'équilibre relatif entre les 1 et les 0 des features de mots est réjouissant.\n",
    "\n",
    "On peut voir que le nombre de tweets positifs représente 1/5 des tweets via la feature seism_association, et on se félicite d'avoir tenté d'équilibrer le jeu de donnés.\n",
    "\n",
    "On peut voir également que le nombre de mots suit une distribution de gauss.\n",
    "\n",
    "La fréquence est répartie de façon homogène tandis que la variation de la fréquence est une demi gaussienne (du à la valeur absolue).\n",
    "\n",
    "Les données nous conviennent on peut maintenant passer à l'étude des corrélations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice des coefficients de Pearson \n",
    "corr_matrix=tweet_clean.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a417be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice des coefficients de Pearson\n",
    "import seaborn as sns\n",
    "heat_map = sns.heatmap(corr_matrix, center=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3820c8",
   "metadata": {},
   "source": [
    "La fréquence est la feature qui présente le plus de corélation avec les labels (seism_association) avec corr=0,5. Ca sera donc une feature très interressante pour prédire les tremblements de terre et on pouvait s'y attendre.\n",
    "\n",
    "Les features concernant le champs lexical du séisme sont corrélés entre elles et ça aussi c'est cohérent. La présence des mots \"tremblements\" et \"terre\" est fortement corrélé aux labels. Pour les mots magnitude et séisme, on retrouve peu de corrélation ce qui s'explique par le fait que sur tweeter, le langage soutenu est moins utilisé. D'un autre coté, les tweets étant mal étiquetés, on pourrait s'attendre à voir les scores de corrélation augmenter pour beaucoup des features ci dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d7fcc",
   "metadata": {},
   "source": [
    "# 7 Normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b73239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des features à normaliser\n",
    "X = tweet_clean.drop('seism_association', axis=1)\n",
    "y = tweet_clean['seism_association']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ced35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # normalisation\n",
    "scaler.fit(X)\n",
    "X_stand = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9da63",
   "metadata": {},
   "source": [
    "# 8. METHODE RANDOMFOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7cf43",
   "metadata": {},
   "source": [
    "Initialisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des entrées du modèle\n",
    "X_train, X_test, y_train, y_test = X_stand[:100000], X_stand[100001:], y[:100000] , y[100001:]\n",
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a86f67",
   "metadata": {},
   "source": [
    "Entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators= 40) # choix de l'hyper paramètre : 40 = Nombre d'arbres de la foret\n",
    "clf.fit(X_train, y_train) # entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a398a",
   "metadata": {},
   "source": [
    "Prédiction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c441ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction sur les X_test à partir de notre modèle entrainé\n",
    "y_predict=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de tweets correspondant à un tremblement de terre selon notre modèle\n",
    "y_predict.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98cff6",
   "metadata": {},
   "source": [
    "La prédiction donne 1 259 tweets positifs contre 3 977 pour les y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6ff30",
   "metadata": {},
   "source": [
    "Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des métriques pour notre modèle entrainé\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(y_test, y_predict))\n",
    "print(recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fab05",
   "metadata": {},
   "source": [
    "25% de précision c'est clairement un score pourris. On peut supposer que si les tweets étaient labéllisés à la main le résultat serait meilleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb44c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict[2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fa459",
   "metadata": {},
   "source": [
    "On observe que le tweet 2 000 a été mal classifié par notre modèle. C'est tout à fait normal vu la faible précision de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173d97a",
   "metadata": {},
   "source": [
    "# Pour synthétiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970b829",
   "metadata": {},
   "source": [
    "On a réussi a labelliser les tweets de façon grossière mais les résultats ne seront pas bon tant que les tweets ne seront pas labellisés de façon plus precise. On pourrait imaginer classifier à la main les 32 000 tweets labellisés positifs par exemple. Sinon, labélliser 32 000 tweets ça coute environ 1 300€ donc c'est abordable.\n",
    "\n",
    "La labellisation des tweets fait apparaitre un paramètre : le temps d'impact d'un séisme sur tweeter. Il vaut 12h dans ce modèle mais cette valeure n'est pas forcément celle optimale.\n",
    "\n",
    "Un rééquilibrage du dataset a permit de travailler avec un dataset présentant 20% de tweets positifs.\n",
    "\n",
    "La feature fréquence fait apparaitre un autre paramètre : la durée autour du tweet pour le calcul de sa fréquence. Elle vaut 12h également et ce paramètre peut être optimisé. Malheureusement, cette feature a un temps de calcul très long mais doit pouvoir s'optimiser facilement. Nous n'avons pas réussi à diminuer le temps de calcul en dessous de 6h de notre coté.\n",
    "\n",
    "La feature variation de fréquence est calculée par une approximation grossière qui ferait peur à un matématicien. Cependant elle nous a permis, une fois implémenté, de passer de 7% d'accuracy à 30% pour certains entrainements de randomforest. Elle fait apparaitre un nouveau paramètre : le pas de calcul pour la variation de fréquence. Il vaut 550 actuellement. Il serait encore plus interessant de remplacer cette feature par un taux d'accroissement. \n",
    "\n",
    "Les features les plus interressantes sont la fréquence, la variation de la fréquence, la présence de mots du champs léxical du séisme. On pourrait étudier d'avantage le texte pour en extraire d'autres features interessantes. Cependant, on remarque que les features telles que le nombre de mots, le nombre d'émojis ou l'analyse de sentiment n'apporte pas vraiment de résultat concluant. En effet, la correlation entre ces features et la présence d'un séisme est très proche de 0.\n",
    "\n",
    "Notre modèle ne nous permet pas de classifier les tweets de façon concluante et nous obtenons au mieux 30% de prédictions justes mais on peut espérer qu'avec une labéllisation plus juste on pourrait obtenir de meilleures prédictions.\n",
    "\n",
    "La piste de recherche la plus prommeteuse serait de passer en Deep Learning en utilisant un réseau de neurones récurent tel que un réseau LSTM : Chaque tweet possède un texte. Une fois nettoyé, ce texte est transformé en un vecteur via de l'embedding. On a donc une séquence de mots transformés en une séquence de nombres. Ainsi, on peut fournir en entrée du réseau LSTM les vecteurs qui sont des séquences correspondant au texte des tweets. Au bout du réseau LSTM, on met une couche de neuronnes dense permettant de classifier de facon binaire le tweets. \n",
    "\n",
    "Cette solution permettrait de traduire tout le texte de chaque tweet en tenant compte de l'ordre des mots des phrases qui est très important pour leur compréhension.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
